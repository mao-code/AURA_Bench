\begin{table*}[h!]
\centering
\small
\setlength{\tabcolsep}{6pt}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{Size} & \textbf{S@10 $\uparrow$} & \textbf{R@10 $\uparrow$} & \textbf{nDCG@10 $\uparrow$} & \textbf{EER@10 $\downarrow$} \\
\midrule
\multicolumn{6}{l}{\textit{LLMs (instruction-tuned)}} \\ 
\midrule
\texttt{llama3-8b-instruct} & 8B & \textbf{0.650} & \textbf{0.647} & \textbf{0.467} & 0.087 \\
\texttt{llama3.1-8b-instruct} & 8B & 0.645 & 0.640 & 0.464 & 0.085 \\
\texttt{qwen2.5-3b-instruct} & 3B & 0.599 & 0.596 & 0.428 & \underline{\textbf{0.080}} \\
\texttt{qwen2.5-7b-instruct} & 7B & 0.609 & 0.605 & 0.427 & 0.082 \\
\texttt{qwen3-4b-instruct} & 4B & 0.593 & 0.586 & 0.415 & 0.089 \\
\midrule
\multicolumn{6}{l}{\textit{LLMs (base)}} \\ 
\midrule
\texttt{llama3-8b} & 8B & \underline{\textbf{0.660}} & \underline{\textbf{0.656}} & \underline{\textbf{0.479}} & 0.089 \\
\texttt{llama3.1-8b} & 8B & 0.659 & 0.655 & 0.476 & 0.089 \\
\texttt{qwen2.5-3b} & 3B & 0.597 & 0.594 & 0.427 & \textbf{0.081} \\
\texttt{qwen3-4b} & 4B & 0.603 & 0.599 & 0.423 & 0.089 \\
\midrule
\multicolumn{6}{l}{\textit{Embedding models (instruction-tuned)}} \\ 
\midrule
\texttt{e5-mistral-7b-instruct} & 7B & 0.605 & 0.598 & 0.426 & 0.102 \\
\texttt{gte-qwen2-7b-instruct} & 7B & \textbf{0.616} & \textbf{0.612} & \textbf{0.438} & \textbf{0.090} \\
\midrule
\multicolumn{6}{l}{\textit{Embedding models}} \\ 
\midrule
\texttt{all-minilm-l12-v2} & \texttt{n/a} & 0.420 & 0.415 & 0.279 & 0.165 \\
\texttt{all-minilm-l6-v2} & \texttt{n/a} & 0.417 & 0.412 & 0.273 & 0.150 \\
\texttt{all-mpnet-base-v2} & \texttt{n/a} & 0.435 & 0.429 & 0.296 & 0.148 \\
\texttt{all-roberta-large-v1} & \texttt{n/a} & 0.492 & 0.487 & 0.329 & 0.128 \\
\texttt{allenai-specter} & \texttt{n/a} & 0.388 & 0.384 & 0.269 & 0.155 \\
\texttt{bert-base-uncased} & \texttt{n/a} & 0.495 & 0.490 & 0.341 & 0.105 \\
\texttt{bge-base-en-v1.5} & \texttt{n/a} & 0.441 & 0.436 & 0.302 & 0.177 \\
\texttt{bge-base-zh-v1.5} & \texttt{n/a} & 0.473 & 0.468 & 0.327 & 0.152 \\
\texttt{bge-large-en-v1.5} & \texttt{n/a} & 0.439 & 0.434 & 0.304 & 0.168 \\
\texttt{bge-large-zh-v1.5} & \texttt{n/a} & 0.449 & 0.442 & 0.317 & 0.165 \\
\texttt{bge-m3} & \texttt{n/a} & 0.392 & 0.388 & 0.282 & 0.273 \\
\texttt{bge-small-en-v1.5} & \texttt{n/a} & 0.417 & 0.414 & 0.288 & 0.193 \\
\texttt{distiluse-base-multilingual-cased-v2} & \texttt{n/a} & 0.409 & 0.404 & 0.291 & 0.224 \\
\texttt{e5-base-v2} & \texttt{n/a} & 0.506 & 0.499 & 0.355 & 0.173 \\
\texttt{e5-large-v2} & \texttt{n/a} & 0.497 & 0.492 & 0.348 & 0.184 \\
\texttt{e5-small-v2} & \texttt{n/a} & 0.465 & 0.461 & 0.322 & 0.216 \\
\texttt{facebook-contriever} & \texttt{n/a} & 0.549 & 0.545 & 0.397 & 0.127 \\
\texttt{facebook-contriever-msmarco} & \texttt{n/a} & 0.457 & 0.454 & 0.319 & 0.179 \\
\texttt{gte-base} & \texttt{n/a} & 0.420 & 0.417 & 0.291 & 0.154 \\
\texttt{gte-large} & \texttt{n/a} & 0.461 & 0.457 & 0.313 & 0.137 \\
\texttt{gte-large-en-v1.5} & \texttt{n/a} & 0.491 & 0.484 & 0.339 & 0.121 \\
\texttt{jina-embeddings-v2-base-en} & \texttt{n/a} & 0.191 & 0.187 & 0.121 & 0.292 \\
\texttt{jina-embeddings-v2-small-en} & \texttt{n/a} & 0.399 & 0.395 & 0.259 & 0.111 \\
\texttt{msmarco-distilbert-base-v4} & \texttt{n/a} & 0.404 & 0.398 & 0.275 & 0.228 \\
\texttt{multilingual-e5-base} & \texttt{n/a} & 0.571 & 0.564 & 0.406 & 0.156 \\
\texttt{multilingual-e5-large} & \texttt{n/a} & 0.582 & 0.577 & \textbf{0.426} & 0.146 \\
\texttt{mxbai-embed-large-v1} & \texttt{n/a} & 0.427 & 0.422 & 0.297 & 0.170 \\
\texttt{nomic-embed-text-v1} & \texttt{n/a} & 0.478 & 0.473 & 0.324 & 0.156 \\
\texttt{nomic-embed-text-v1.5} & \texttt{n/a} & 0.461 & 0.456 & 0.320 & 0.135 \\
\texttt{paraphrase-mpnet-base-v2} & \texttt{n/a} & 0.445 & 0.438 & 0.299 & 0.148 \\
\texttt{paraphrase-multilingual-mpnet-base-v2} & \texttt{n/a} & 0.348 & 0.345 & 0.248 & 0.258 \\
\texttt{qwen3-embedding-0.6b} & 0.6B & 0.539 & 0.533 & 0.368 & 0.113 \\
\texttt{qwen3-embedding-4b} & 4B & 0.562 & 0.558 & 0.390 & 0.110 \\
\texttt{qwen3-embedding-8b} & 8B & 0.588 & 0.581 & 0.409 & 0.127 \\
\texttt{sfr-embedding-mistral} & 7B & \textbf{0.592} & \textbf{0.583} & 0.421 & \textbf{0.101} \\
\texttt{snowflake-arctic-embed-l-v2} & \texttt{n/a} & 0.441 & 0.438 & 0.313 & 0.203 \\
\midrule
\multicolumn{6}{l}{\textit{Lexical baseline}} \\ 
\midrule
\texttt{TF-IDF} & \texttt{n/a} & \textbf{0.409} & \textbf{0.406} & \textbf{0.287} & \textbf{0.180} \\
\midrule
\bottomrule
\end{tabular}
}
\caption{\textbf{Overall results on AURA-Bench.}
Authorship representation is evaluated with Success@10 (S@10), Recall@10 (R@10), and nDCG@10 (higher is better).
Authorship verification is evaluated with equal error rate (EER@10; lower is better).
Within each category block, best values are bolded (max for S/R/nDCG; min for EER). Across category blocks, best values are underlined.
\textbf{Size} denotes parameter count when publicly available; proprietary hosted models may not disclose sizes (marked \texttt{n/a}).}
\label{tab:overall-leaderboard-full}
\end{table*}

\begin{table*}[h!]
\centering
\small
\setlength{\tabcolsep}{6pt}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccccccccc}
\toprule
\textbf{Model} & \textbf{Size} & \textbf{\texttt{ar}} & \textbf{\texttt{de}} & \textbf{\texttt{en}} & \textbf{\texttt{es}} & \textbf{\texttt{fr}} & \textbf{\texttt{hi}} & \textbf{\texttt{ja}} & \textbf{\texttt{ko}} & \textbf{\texttt{ru}} & \textbf{\texttt{zh}} \\
\midrule
\multicolumn{12}{l}{\textit{LLMs (instruction-tuned)}} \\ 
\midrule
\texttt{llama3-8b-instruct} & 8B & 0.508 & \textbf{0.862} & \textbf{0.616} & 0.752 & 0.742 & \underline{\textbf{0.900}} & \underline{\textbf{1.000}} & 0.598 & \textbf{0.617} & 0.614 \\
\texttt{llama3.1-8b-instruct} & 8B & \underline{\textbf{0.513}} & \textbf{0.862} & \textbf{0.616} & \textbf{0.762} & \textbf{0.758} & 0.875 & \underline{\textbf{1.000}} & 0.587 & 0.594 & 0.591 \\
\texttt{qwen2.5-3b-instruct} & 3B & 0.487 & 0.769 & 0.479 & 0.693 & 0.694 & 0.775 & 0.941 & 0.565 & 0.602 & 0.614 \\
\texttt{qwen2.5-7b-instruct} & 7B & 0.476 & 0.785 & 0.548 & 0.703 & 0.710 & 0.750 & \underline{\textbf{1.000}} & \underline{\textbf{0.620}} & 0.541 & 0.630 \\
\texttt{qwen3-4b-instruct} & 4B & 0.476 & 0.738 & 0.479 & 0.673 & 0.661 & 0.750 & 0.882 & 0.609 & 0.579 & \textbf{0.638} \\
\midrule
\multicolumn{12}{l}{\textit{LLMs (base)}} \\ 
\midrule
\texttt{llama3-8b} & 8B & 0.508 & \underline{\textbf{0.877}} & \underline{\textbf{0.630}} & 0.752 & 0.774 & 0.875 & \textbf{0.941} & \underline{\textbf{0.620}} & \underline{\textbf{0.647}} & 0.622 \\
\texttt{llama3.1-8b} & 8B & 0.508 & \underline{\textbf{0.877}} & \underline{\textbf{0.630}} & \underline{\textbf{0.782}} & \underline{\textbf{0.823}} & \underline{\textbf{0.900}} & \textbf{0.941} & 0.609 & 0.617 & 0.598 \\
\texttt{qwen2.5-3b} & 3B & 0.497 & 0.754 & 0.473 & 0.683 & 0.677 & 0.725 & \textbf{0.941} & 0.576 & 0.594 & 0.630 \\
\texttt{qwen3-4b} & 4B & \underline{\textbf{0.513}} & 0.754 & 0.534 & 0.673 & 0.677 & 0.750 & 0.882 & 0.565 & 0.556 & \textbf{0.638} \\
\midrule
\multicolumn{12}{l}{\textit{Embedding models (instruction-tuned)}} \\ 
\midrule
\texttt{e5-mistral-7b-instruct} & 7B & \textbf{0.460} & 0.769 & \textbf{0.562} & \textbf{0.713} & 0.661 & 0.775 & \underline{\textbf{1.000}} & 0.554 & 0.594 & \textbf{0.614} \\
\texttt{gte-qwen2-7b-instruct} & 7B & 0.455 & \textbf{0.831} & 0.548 & 0.703 & \textbf{0.710} & \textbf{0.850} & \underline{\textbf{1.000}} & \textbf{0.576} & \textbf{0.617} & \textbf{0.614} \\
\midrule
\multicolumn{12}{l}{\textit{Embedding models}} \\ 
\midrule
\texttt{all-minilm-l12-v2} & \texttt{n/a} & 0.296 & 0.477 & 0.411 & 0.495 & 0.484 & 0.725 & 0.588 & 0.435 & 0.383 & 0.402 \\
\texttt{all-minilm-l6-v2} & \texttt{n/a} & 0.307 & 0.538 & 0.384 & 0.505 & 0.403 & 0.650 & 0.471 & 0.424 & 0.383 & 0.441 \\
\texttt{all-mpnet-base-v2} & \texttt{n/a} & 0.323 & 0.600 & 0.473 & 0.495 & 0.435 & 0.750 & 0.529 & 0.413 & 0.323 & 0.449 \\
\texttt{all-roberta-large-v1} & \texttt{n/a} & 0.434 & 0.723 & 0.486 & 0.614 & 0.613 & 0.775 & 0.765 & 0.337 & 0.391 & 0.402 \\
\texttt{allenai-specter} & \texttt{n/a} & 0.238 & 0.738 & 0.363 & 0.554 & 0.532 & 0.375 & 0.353 & 0.391 & 0.331 & 0.323 \\
\texttt{bert-base-uncased} & \texttt{n/a} & 0.333 & 0.677 & 0.514 & 0.545 & 0.532 & 0.725 & 0.882 & 0.522 & 0.451 & 0.465 \\
\texttt{bge-base-en-v1.5} & \texttt{n/a} & 0.312 & 0.677 & 0.384 & 0.604 & 0.468 & 0.675 & 0.765 & 0.402 & 0.383 & 0.409 \\
\texttt{bge-base-zh-v1.5} & \texttt{n/a} & 0.328 & 0.585 & 0.404 & 0.495 & 0.452 & 0.775 & 0.882 & \textbf{0.565} & 0.398 & 0.567 \\
\texttt{bge-large-en-v1.5} & \texttt{n/a} & 0.243 & 0.677 & 0.432 & 0.614 & 0.484 & 0.675 & 0.471 & 0.435 & 0.391 & 0.433 \\
\texttt{bge-large-zh-v1.5} & \texttt{n/a} & 0.323 & 0.646 & 0.377 & 0.485 & 0.419 & 0.700 & 0.824 & 0.467 & 0.368 & 0.543 \\
\texttt{bge-m3} & \texttt{n/a} & 0.354 & 0.523 & 0.295 & 0.614 & 0.387 & 0.425 & 0.235 & 0.304 & 0.353 & 0.433 \\
\texttt{bge-small-en-v1.5} & \texttt{n/a} & 0.217 & 0.554 & 0.486 & 0.545 & 0.484 & 0.625 & 0.588 & 0.413 & 0.361 & 0.402 \\
\texttt{distiluse-base-multilingual-cased-v2} & \texttt{n/a} & 0.365 & 0.523 & 0.418 & 0.535 & 0.339 & 0.600 & 0.412 & 0.402 & 0.278 & 0.425 \\
\texttt{e5-base-v2} & \texttt{n/a} & 0.376 & \textbf{0.754} & 0.521 & 0.604 & 0.565 & \textbf{0.850} & 0.765 & 0.457 & 0.391 & 0.465 \\
\texttt{e5-large-v2} & \texttt{n/a} & 0.360 & 0.615 & 0.466 & 0.624 & 0.581 & 0.800 & 0.765 & 0.522 & 0.444 & 0.441 \\
\texttt{e5-small-v2} & \texttt{n/a} & 0.344 & 0.600 & 0.445 & 0.525 & 0.452 & 0.775 & 0.765 & 0.467 & 0.429 & 0.457 \\
\texttt{facebook-contriever} & \texttt{n/a} & 0.381 & 0.708 & \textbf{0.596} & \textbf{0.743} & 0.661 & 0.775 & 0.765 & 0.511 & 0.466 & 0.472 \\
\texttt{facebook-contriever-msmarco} & \texttt{n/a} & 0.349 & 0.600 & 0.459 & 0.495 & 0.435 & 0.700 & 0.706 & 0.435 & 0.444 & 0.441 \\
\texttt{gte-base} & \texttt{n/a} & 0.222 & 0.677 & 0.411 & 0.535 & 0.516 & 0.625 & 0.706 & 0.402 & 0.376 & 0.409 \\
\texttt{gte-large} & \texttt{n/a} & 0.270 & 0.708 & 0.459 & 0.644 & 0.548 & 0.775 & 0.647 & 0.391 & 0.421 & 0.402 \\
\texttt{gte-large-en-v1.5} & \texttt{n/a} & 0.344 & 0.738 & 0.459 & 0.604 & 0.613 & 0.750 & 0.647 & 0.413 & 0.429 & 0.488 \\
\texttt{jina-embeddings-v2-base-en} & \texttt{n/a} & 0.259 & 0.138 & 0.062 & 0.119 & 0.065 & 0.425 & 0.412 & 0.217 & 0.135 & 0.323 \\
\texttt{jina-embeddings-v2-small-en} & \texttt{n/a} & 0.270 & 0.508 & 0.438 & 0.396 & 0.339 & 0.725 & 0.882 & 0.424 & 0.338 & 0.402 \\
\texttt{msmarco-distilbert-base-v4} & \texttt{n/a} & 0.323 & 0.600 & 0.370 & 0.436 & 0.355 & 0.725 & 0.647 & 0.457 & 0.323 & 0.378 \\
\texttt{multilingual-e5-base} & \texttt{n/a} & 0.481 & 0.738 & 0.514 & 0.683 & 0.645 & 0.725 & 0.647 & 0.500 & 0.564 & 0.559 \\
\texttt{multilingual-e5-large} & \texttt{n/a} & \textbf{0.492} & \textbf{0.754} & 0.514 & 0.683 & 0.710 & 0.700 & 0.824 & 0.467 & \textbf{0.586} & 0.575 \\
\texttt{mxbai-embed-large-v1} & \texttt{n/a} & 0.238 & 0.677 & 0.418 & 0.564 & 0.500 & 0.725 & 0.471 & 0.413 & 0.368 & 0.417 \\
\texttt{nomic-embed-text-v1} & \texttt{n/a} & 0.307 & 0.738 & 0.452 & 0.653 & 0.597 & 0.725 & 0.647 & 0.391 & 0.406 & 0.472 \\
\texttt{nomic-embed-text-v1.5} & \texttt{n/a} & 0.286 & 0.738 & 0.452 & 0.644 & 0.565 & 0.700 & 0.706 & 0.380 & 0.368 & 0.441 \\
\texttt{paraphrase-mpnet-base-v2} & \texttt{n/a} & 0.344 & 0.585 & 0.479 & 0.485 & 0.419 & 0.625 & 0.765 & 0.467 & 0.383 & 0.417 \\
\texttt{paraphrase-multilingual-mpnet-base-v2} & \texttt{n/a} & 0.296 & 0.431 & 0.363 & 0.426 & 0.210 & 0.525 & 0.294 & 0.293 & 0.338 & 0.370 \\
\texttt{qwen3-embedding-0.6b} & 0.6B & 0.429 & 0.677 & 0.459 & 0.614 & 0.645 & 0.675 & 0.941 & 0.522 & 0.489 & 0.583 \\
\texttt{qwen3-embedding-4b} & 4B & 0.402 & \textbf{0.754} & 0.521 & 0.653 & \textbf{0.726} & 0.750 & 0.882 & 0.522 & 0.496 & 0.591 \\
\texttt{qwen3-embedding-8b} & 8B & 0.487 & 0.692 & 0.493 & 0.653 & 0.661 & 0.700 & 0.941 & \textbf{0.565} & 0.579 & \underline{\textbf{0.654}} \\
\texttt{sfr-embedding-mistral} & 7B & 0.429 & 0.738 & 0.568 & 0.693 & 0.661 & 0.800 & \underline{\textbf{1.000}} & \textbf{0.565} & 0.579 & 0.583 \\
\texttt{snowflake-arctic-embed-l-v2} & \texttt{n/a} & 0.402 & 0.477 & 0.425 & 0.554 & 0.419 & 0.675 & 0.176 & 0.380 & 0.391 & 0.480 \\
\midrule
\multicolumn{12}{l}{\textit{Lexical baseline}} \\ 
\midrule
\texttt{TF-IDF} & \texttt{n/a} & \textbf{0.434} & \textbf{0.508} & \textbf{0.315} & \textbf{0.337} & \textbf{0.484} & \textbf{0.625} & \textbf{0.529} & \textbf{0.478} & \textbf{0.414} & \textbf{0.315} \\
\midrule
\bottomrule
\end{tabular}
}
\caption{\textbf{Language-wise Success@10 on AURA-Bench (full results).} Authorship representation is evaluated as retrieval with Success@10 (higher is better). Within each category block, best values are bolded (max for Success@10). Across all models, best values are underlined. \textbf{Size} denotes parameter count when publicly available; proprietary hosted models may not disclose sizes (marked \texttt{n/a}).}
\label{tab:lang-s10-full}
\end{table*}

\begin{table*}[h!]
\centering
\small
\setlength{\tabcolsep}{6pt}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccccccccc}
\toprule
\textbf{Model} & \textbf{Size} & \textbf{\texttt{ar}} & \textbf{\texttt{de}} & \textbf{\texttt{en}} & \textbf{\texttt{es}} & \textbf{\texttt{fr}} & \textbf{\texttt{hi}} & \textbf{\texttt{ja}} & \textbf{\texttt{ko}} & \textbf{\texttt{ru}} & \textbf{\texttt{zh}} \\
\midrule
\multicolumn{12}{l}{\textit{LLMs (instruction-tuned)}} \\ 
\midrule
\texttt{llama3-8b-instruct} & 8B & 0.086 & \textbf{0.038} & \textbf{0.081} & 0.065 & \underline{\textbf{0.046}} & \textbf{0.025} & \textbf{0.013} & \textbf{0.057} & 0.070 & 0.068 \\
\texttt{llama3.1-8b-instruct} & 8B & 0.086 & 0.045 & 0.082 & 0.059 & 0.048 & 0.026 & 0.014 & 0.062 & 0.069 & 0.071 \\
\texttt{qwen2.5-3b-instruct} & 3B & \underline{\textbf{0.082}} & 0.046 & 0.101 & 0.059 & 0.056 & 0.038 & 0.014 & 0.075 & \textbf{0.067} & 0.068 \\
\texttt{qwen2.5-7b-instruct} & 7B & 0.086 & 0.046 & 0.088 & 0.059 & 0.059 & 0.033 & \textbf{0.013} & 0.058 & 0.075 & 0.069 \\
\texttt{qwen3-4b-instruct} & 4B & 0.096 & 0.046 & 0.101 & \underline{\textbf{0.058}} & 0.062 & 0.033 & 0.014 & 0.073 & 0.073 & \textbf{0.068} \\
\midrule
\multicolumn{12}{l}{\textit{LLMs (base)}} \\ 
\midrule
\texttt{llama3-8b} & 8B & 0.091 & 0.039 & 0.074 & \textbf{0.059} & 0.048 & \underline{\textbf{0.025}} & \textbf{0.014} & 0.052 & 0.068 & \textbf{0.069} \\
\texttt{llama3.1-8b} & 8B & \textbf{0.086} & \underline{\textbf{0.033}} & \underline{\textbf{0.073}} & 0.062 & \textbf{0.047} & \underline{\textbf{0.025}} & 0.015 & \underline{\textbf{0.052}} & \textbf{0.063} & 0.074 \\
\texttt{qwen2.5-3b} & 3B & 0.089 & 0.046 & 0.100 & 0.060 & 0.057 & 0.032 & \textbf{0.014} & 0.072 & 0.066 & 0.072 \\
\texttt{qwen3-4b} & 4B & 0.093 & \underline{\textbf{0.033}} & 0.094 & 0.059 & 0.060 & 0.029 & \textbf{0.014} & 0.077 & 0.072 & 0.075 \\
\midrule
\multicolumn{12}{l}{\textit{Embedding models (instruction-tuned)}} \\ 
\midrule
\texttt{e5-mistral-7b-instruct} & 7B & \textbf{0.101} & \textbf{0.046} & 0.088 & 0.070 & \textbf{0.051} & 0.035 & 0.015 & 0.072 & 0.073 & 0.075 \\
\texttt{gte-qwen2-7b-instruct} & 7B & 0.103 & \textbf{0.046} & \textbf{0.079} & \textbf{0.065} & 0.053 & \textbf{0.027} & \textbf{0.014} & \textbf{0.072} & \underline{\textbf{0.062}} & \underline{\textbf{0.066}} \\
\midrule
\multicolumn{12}{l}{\textit{Embedding models}} \\ 
\midrule
\texttt{all-minilm-l12-v2} & \texttt{n/a} & 0.139 & 0.185 & 0.250 & 0.075 & 0.108 & 0.059 & 0.111 & 0.103 & 0.097 & 0.117 \\
\texttt{all-minilm-l6-v2} & \texttt{n/a} & 0.130 & 0.149 & 0.264 & 0.084 & 0.092 & 0.064 & 0.081 & 0.105 & 0.077 & 0.121 \\
\texttt{all-mpnet-base-v2} & \texttt{n/a} & 0.131 & 0.108 & 0.243 & 0.088 & 0.123 & 0.050 & 0.111 & 0.099 & 0.081 & 0.123 \\
\texttt{all-roberta-large-v1} & \texttt{n/a} & 0.107 & 0.083 & 0.209 & 0.071 & 0.077 & 0.054 & 0.111 & 0.080 & 0.088 & 0.105 \\
\texttt{allenai-specter} & \texttt{n/a} & 0.211 & 0.077 & 0.214 & 0.089 & 0.108 & 0.175 & 0.167 & 0.140 & 0.117 & 0.180 \\
\texttt{bert-base-uncased} & \texttt{n/a} & 0.128 & 0.056 & 0.139 & 0.075 & 0.062 & 0.035 & 0.056 & 0.087 & 0.088 & 0.111 \\
\texttt{bge-base-en-v1.5} & \texttt{n/a} & 0.136 & 0.123 & 0.358 & 0.103 & 0.169 & 0.037 & 0.064 & 0.107 & 0.094 & 0.143 \\
\texttt{bge-base-zh-v1.5} & \texttt{n/a} & 0.136 & 0.062 & 0.197 & 0.084 & 0.122 & 0.075 & 0.018 & 0.091 & 0.102 & 0.188 \\
\texttt{bge-large-en-v1.5} & \texttt{n/a} & 0.127 & 0.108 & 0.324 & 0.123 & 0.145 & 0.033 & 0.045 & 0.095 & 0.109 & 0.114 \\
\texttt{bge-large-zh-v1.5} & \texttt{n/a} & 0.135 & 0.092 & 0.269 & 0.110 & 0.154 & 0.100 & 0.111 & 0.124 & 0.117 & 0.248 \\
\texttt{bge-m3} & \texttt{n/a} & 0.242 & 0.258 & 0.324 & 0.159 & 0.231 & 0.300 & 0.333 & 0.324 & 0.321 & 0.263 \\
\texttt{bge-small-en-v1.5} & \texttt{n/a} & 0.157 & 0.096 & 0.250 & 0.130 & 0.123 & 0.050 & 0.049 & 0.161 & 0.103 & 0.143 \\
\texttt{distiluse-base-multilingual-cased-v2} & \texttt{n/a} & 0.187 & 0.198 & 0.209 & 0.169 & 0.246 & 0.192 & 0.167 & 0.258 & 0.274 & 0.256 \\
\texttt{e5-base-v2} & \texttt{n/a} & 0.107 & 0.112 & 0.264 & 0.100 & 0.162 & 0.074 & 0.054 & 0.113 & 0.133 & 0.135 \\
\texttt{e5-large-v2} & \texttt{n/a} & 0.131 & 0.155 & 0.230 & 0.121 & 0.165 & 0.068 & 0.111 & 0.113 & 0.188 & 0.171 \\
\texttt{e5-small-v2} & \texttt{n/a} & 0.132 & 0.215 & 0.318 & 0.173 & 0.215 & 0.084 & 0.133 & 0.133 & 0.190 & 0.180 \\
\texttt{facebook-contriever} & \texttt{n/a} & 0.099 & 0.077 & 0.174 & 0.065 & 0.092 & 0.029 & 0.056 & 0.082 & 0.080 & 0.105 \\
\texttt{facebook-contriever-msmarco} & \texttt{n/a} & 0.133 & 0.123 & 0.250 & 0.112 & 0.185 & 0.054 & 0.056 & 0.093 & 0.102 & 0.119 \\
\texttt{gte-base} & \texttt{n/a} & 0.136 & 0.077 & 0.297 & 0.084 & 0.097 & 0.038 & 0.054 & 0.079 & 0.084 & 0.129 \\
\texttt{gte-large} & \texttt{n/a} & 0.121 & 0.061 & 0.270 & 0.079 & 0.068 & 0.030 & 0.031 & 0.082 & 0.095 & 0.109 \\
\texttt{gte-large-en-v1.5} & \texttt{n/a} & 0.111 & 0.052 & 0.291 & 0.074 & 0.059 & 0.037 & 0.084 & 0.111 & 0.102 & 0.120 \\
\texttt{jina-embeddings-v2-base-en} & \texttt{n/a} & 0.172 & 0.256 & 0.325 & 0.271 & 0.262 & 0.175 & 0.261 & 0.361 & 0.277 & 0.203 \\
\texttt{jina-embeddings-v2-small-en} & \texttt{n/a} & 0.114 & 0.109 & 0.135 & 0.112 & 0.123 & \underline{\textbf{0.025}} & 0.056 & 0.093 & 0.097 & 0.098 \\
\texttt{msmarco-distilbert-base-v4} & \texttt{n/a} & 0.147 & 0.097 & 0.262 & 0.081 & 0.121 & 0.034 & 0.167 & 0.134 & 0.115 & 0.124 \\
\texttt{multilingual-e5-base} & \texttt{n/a} & 0.154 & 0.154 & 0.186 & 0.104 & 0.119 & 0.125 & 0.167 & 0.105 & 0.226 & 0.091 \\
\texttt{multilingual-e5-large} & \texttt{n/a} & 0.152 & 0.154 & 0.162 & 0.097 & 0.100 & 0.089 & 0.111 & 0.134 & 0.206 & 0.078 \\
\texttt{mxbai-embed-large-v1} & \texttt{n/a} & 0.131 & 0.119 & 0.324 & 0.112 & 0.135 & 0.032 & 0.056 & 0.085 & 0.104 & 0.117 \\
\texttt{nomic-embed-text-v1} & \texttt{n/a} & 0.120 & 0.058 & 0.237 & 0.065 & 0.084 & 0.044 & 0.056 & 0.113 & 0.083 & 0.135 \\
\texttt{nomic-embed-text-v1.5} & \texttt{n/a} & 0.118 & 0.049 & 0.216 & \textbf{0.062} & \textbf{0.049} & 0.037 & 0.056 & 0.093 & 0.083 & 0.124 \\
\texttt{paraphrase-mpnet-base-v2} & \texttt{n/a} & 0.145 & 0.077 & 0.196 & 0.085 & 0.093 & 0.107 & 0.111 & 0.124 & 0.102 & 0.120 \\
\texttt{paraphrase-multilingual-mpnet-base-v2} & \texttt{n/a} & 0.249 & 0.200 & 0.297 & 0.215 & 0.262 & 0.225 & 0.278 & 0.294 & 0.307 & 0.230 \\
\texttt{qwen3-embedding-0.6b} & 0.6B & 0.136 & 0.108 & 0.128 & 0.100 & 0.062 & 0.088 & \underline{\textbf{0.008}} & 0.081 & 0.080 & 0.098 \\
\texttt{qwen3-embedding-4b} & 4B & 0.109 & 0.092 & 0.141 & 0.093 & 0.092 & 0.075 & 0.014 & 0.080 & 0.095 & 0.120 \\
\texttt{qwen3-embedding-8b} & 8B & \textbf{0.096} & 0.106 & 0.162 & 0.123 & 0.121 & 0.039 & \underline{\textbf{0.008}} & 0.078 & 0.095 & 0.091 \\
\texttt{sfr-embedding-mistral} & 7B & 0.097 & \textbf{0.046} & \textbf{0.085} & 0.070 & 0.052 & 0.036 & 0.020 & \textbf{0.072} & \textbf{0.073} & \textbf{0.075} \\
\texttt{snowflake-arctic-embed-l-v2} & \texttt{n/a} & 0.172 & 0.200 & 0.229 & 0.150 & 0.246 & 0.091 & 0.301 & 0.237 & 0.241 & 0.188 \\
\midrule
\multicolumn{12}{l}{\textit{Lexical baseline}} \\ 
\midrule
\texttt{TF-IDF} & \texttt{n/a} & \textbf{0.115} & \textbf{0.080} & \textbf{0.203} & \textbf{0.115} & \textbf{0.138} & \textbf{0.100} & \textbf{0.333} & \textbf{0.392} & \textbf{0.102} & \textbf{0.571} \\
\midrule
\bottomrule
\end{tabular}
}
\caption{\textbf{Language-wise EER@10 on AURA-Bench (full results).} Authorship verification is evaluated with equal error rate (EER@10; lower is better). Within each category block, best values are bolded (min for EER@10). Across all models, best values are underlined. \textbf{Size} denotes parameter count when publicly available; proprietary hosted models may not disclose sizes (marked \texttt{n/a}).}
\label{tab:lang-eer-full}
\end{table*}

\begin{table*}[h!]
\centering
\small
\setlength{\tabcolsep}{6pt}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccccccc}
\toprule
\textbf{Model} & \textbf{Size} & \textbf{\texttt{blog}} & \textbf{\texttt{ecommerce\_reviews}} & \textbf{\texttt{literature}} & \textbf{\texttt{media\_reviews}} & \textbf{\texttt{news}} & \textbf{\texttt{poetry}} & \textbf{\texttt{research\_paper}} & \textbf{\texttt{social\_media}} \\
\midrule
\multicolumn{10}{l}{\textit{LLMs (instruction-tuned)}} \\ 
\midrule
\texttt{llama3-8b-instruct} & 8B & \textbf{0.440} & 0.487 & \textbf{0.762} & 0.345 & \underline{\textbf{0.690}} & 0.632 & \underline{\textbf{1.000}} & \textbf{0.705} \\
\texttt{llama3.1-8b-instruct} & 8B & 0.434 & \textbf{0.513} & 0.756 & 0.291 & 0.675 & \underline{\textbf{0.684}} & \underline{\textbf{1.000}} & 0.702 \\
\texttt{qwen2.5-3b-instruct} & 3B & 0.244 & 0.436 & 0.708 & 0.309 & 0.621 & 0.526 & 0.800 & 0.684 \\
\texttt{qwen2.5-7b-instruct} & 7B & 0.389 & 0.487 & 0.708 & 0.345 & 0.650 & 0.500 & \underline{\textbf{1.000}} & 0.661 \\
\texttt{qwen3-4b-instruct} & 4B & 0.228 & 0.462 & 0.696 & \underline{\textbf{0.400}} & 0.581 & 0.526 & \underline{\textbf{1.000}} & 0.680 \\
\midrule
\multicolumn{10}{l}{\textit{LLMs (base)}} \\ 
\midrule
\texttt{llama3-8b} & 8B & \underline{\textbf{0.501}} & \underline{\textbf{0.538}} & 0.780 & 0.345 & \underline{\textbf{0.690}} & 0.605 & \underline{\textbf{1.000}} & \underline{\textbf{0.717}} \\
\texttt{llama3.1-8b} & 8B & 0.440 & \underline{\textbf{0.538}} & \underline{\textbf{0.792}} & 0.327 & 0.686 & 0.605 & \underline{\textbf{1.000}} & 0.709 \\
\texttt{qwen2.5-3b} & 3B & 0.229 & 0.436 & 0.702 & 0.345 & 0.621 & 0.526 & \underline{\textbf{1.000}} & 0.678 \\
\texttt{qwen3-4b} & 4B & 0.312 & 0.513 & 0.714 & \textbf{0.382} & 0.603 & \textbf{0.632} & 0.800 & 0.662 \\
\midrule
\multicolumn{10}{l}{\textit{Embedding models (instruction-tuned)}} \\ 
\midrule
\texttt{e5-mistral-7b-instruct} & 7B & \textbf{0.423} & 0.385 & 0.732 & \textbf{0.364} & 0.643 & \textbf{0.500} & \underline{\textbf{1.000}} & 0.632 \\
\texttt{gte-qwen2-7b-instruct} & 7B & 0.396 & \textbf{0.462} & \textbf{0.756} & 0.327 & \textbf{0.664} & \textbf{0.500} & \underline{\textbf{1.000}} & \textbf{0.653} \\
\midrule
\multicolumn{10}{l}{\textit{Embedding models}} \\ 
\midrule
\texttt{all-minilm-l12-v2} & \texttt{n/a} & 0.317 & 0.282 & 0.536 & 0.145 & 0.394 & 0.158 & \underline{\textbf{1.000}} & 0.463 \\
\texttt{all-minilm-l6-v2} & \texttt{n/a} & 0.340 & 0.231 & 0.542 & 0.218 & 0.397 & 0.132 & \underline{\textbf{1.000}} & 0.473 \\
\texttt{all-mpnet-base-v2} & \texttt{n/a} & \textbf{0.479} & 0.385 & 0.536 & 0.200 & 0.397 & 0.289 & \underline{\textbf{1.000}} & 0.460 \\
\texttt{all-roberta-large-v1} & \texttt{n/a} & 0.362 & 0.487 & 0.613 & 0.200 & 0.422 & 0.500 & \underline{\textbf{1.000}} & 0.512 \\
\texttt{allenai-specter} & \texttt{n/a} & 0.318 & 0.231 & 0.613 & 0.091 & 0.375 & 0.105 & \underline{\textbf{1.000}} & 0.398 \\
\texttt{bert-base-uncased} & \texttt{n/a} & 0.339 & 0.410 & 0.637 & 0.182 & 0.534 & 0.263 & \underline{\textbf{1.000}} & 0.518 \\
\texttt{bge-base-en-v1.5} & \texttt{n/a} & 0.261 & 0.308 & 0.655 & 0.200 & 0.375 & 0.211 & \underline{\textbf{1.000}} & 0.509 \\
\texttt{bge-base-zh-v1.5} & \texttt{n/a} & 0.268 & 0.231 & 0.524 & 0.327 & 0.516 & 0.158 & \underline{\textbf{1.000}} & 0.542 \\
\texttt{bge-large-en-v1.5} & \texttt{n/a} & 0.350 & 0.359 & 0.649 & 0.200 & 0.372 & 0.263 & \underline{\textbf{1.000}} & 0.471 \\
\texttt{bge-large-zh-v1.5} & \texttt{n/a} & 0.278 & 0.359 & 0.518 & 0.273 & 0.473 & 0.289 & \underline{\textbf{1.000}} & 0.504 \\
\texttt{bge-m3} & \texttt{n/a} & 0.223 & 0.205 & 0.595 & 0.182 & 0.339 & 0.447 & \underline{\textbf{1.000}} & 0.418 \\
\texttt{bge-small-en-v1.5} & \texttt{n/a} & 0.396 & 0.436 & 0.595 & 0.182 & 0.354 & 0.184 & \underline{\textbf{1.000}} & 0.508 \\
\texttt{distiluse-base-multilingual-cased-v2} & \texttt{n/a} & 0.267 & 0.359 & 0.470 & 0.182 & 0.401 & 0.368 & \underline{\textbf{1.000}} & 0.431 \\
\texttt{e5-base-v2} & \texttt{n/a} & 0.401 & 0.513 & 0.643 & 0.218 & 0.487 & 0.395 & \underline{\textbf{1.000}} & 0.524 \\
\texttt{e5-large-v2} & \texttt{n/a} & 0.294 & 0.513 & 0.643 & 0.200 & 0.495 & 0.184 & \underline{\textbf{1.000}} & 0.533 \\
\texttt{e5-small-v2} & \texttt{n/a} & 0.295 & 0.487 & 0.607 & 0.182 & 0.437 & 0.158 & 0.800 & 0.538 \\
\texttt{facebook-contriever} & \texttt{n/a} & 0.383 & 0.487 & 0.726 & 0.255 & 0.556 & 0.316 & \underline{\textbf{1.000}} & 0.554 \\
\texttt{facebook-contriever-msmarco} & \texttt{n/a} & 0.267 & 0.333 & 0.577 & 0.273 & 0.440 & 0.368 & \underline{\textbf{1.000}} & 0.514 \\
\texttt{gte-base} & \texttt{n/a} & 0.289 & 0.256 & 0.613 & 0.236 & 0.383 & 0.079 & \underline{\textbf{1.000}} & 0.482 \\
\texttt{gte-large} & \texttt{n/a} & 0.383 & 0.308 & 0.661 & 0.200 & 0.419 & 0.263 & \underline{\textbf{1.000}} & 0.501 \\
\texttt{gte-large-en-v1.5} & \texttt{n/a} & 0.467 & 0.179 & 0.661 & 0.218 & 0.516 & 0.368 & \underline{\textbf{1.000}} & 0.461 \\
\texttt{jina-embeddings-v2-base-en} & \texttt{n/a} & 0.016 & 0.051 & 0.125 & 0.200 & 0.162 & 0.263 & 0.000 & 0.302 \\
\texttt{jina-embeddings-v2-small-en} & \texttt{n/a} & 0.306 & 0.333 & 0.387 & 0.127 & 0.458 & 0.158 & \underline{\textbf{1.000}} & 0.439 \\
\texttt{msmarco-distilbert-base-v4} & \texttt{n/a} & 0.250 & 0.282 & 0.494 & 0.164 & 0.401 & 0.184 & \underline{\textbf{1.000}} & 0.465 \\
\texttt{multilingual-e5-base} & \texttt{n/a} & 0.300 & \underline{\textbf{0.538}} & 0.696 & 0.291 & 0.585 & 0.500 & \underline{\textbf{1.000}} & \textbf{0.628} \\
\texttt{multilingual-e5-large} & \texttt{n/a} & 0.384 & \underline{\textbf{0.538}} & 0.708 & 0.291 & 0.617 & 0.500 & \underline{\textbf{1.000}} & 0.581 \\
\texttt{mxbai-embed-large-v1} & \texttt{n/a} & 0.350 & 0.333 & 0.631 & 0.200 & 0.357 & 0.184 & \underline{\textbf{1.000}} & 0.473 \\
\texttt{nomic-embed-text-v1} & \texttt{n/a} & 0.340 & 0.359 & 0.726 & 0.255 & 0.390 & 0.211 & \underline{\textbf{1.000}} & 0.517 \\
\texttt{nomic-embed-text-v1.5} & \texttt{n/a} & 0.340 & 0.410 & 0.696 & 0.218 & 0.368 & 0.237 & \underline{\textbf{1.000}} & 0.503 \\
\texttt{paraphrase-mpnet-base-v2} & \texttt{n/a} & 0.362 & \underline{\textbf{0.538}} & 0.554 & 0.182 & 0.415 & 0.211 & \underline{\textbf{1.000}} & 0.457 \\
\texttt{paraphrase-multilingual-mpnet-base-v2} & \texttt{n/a} & 0.256 & 0.385 & 0.440 & 0.145 & 0.296 & 0.263 & 0.800 & 0.377 \\
\texttt{qwen3-embedding-0.6b} & 0.6B & 0.368 & 0.410 & 0.696 & 0.327 & 0.560 & 0.368 & \underline{\textbf{1.000}} & 0.604 \\
\texttt{qwen3-embedding-4b} & 4B & 0.395 & 0.462 & \textbf{0.744} & 0.345 & 0.570 & 0.421 & \underline{\textbf{1.000}} & 0.593 \\
\texttt{qwen3-embedding-8b} & 8B & 0.373 & 0.487 & 0.702 & \underline{\textbf{0.400}} & \textbf{0.650} & \textbf{0.526} & 0.800 & 0.587 \\
\texttt{sfr-embedding-mistral} & 7B & 0.423 & 0.410 & 0.726 & 0.327 & 0.632 & 0.447 & \underline{\textbf{1.000}} & 0.606 \\
\texttt{snowflake-arctic-embed-l-v2} & \texttt{n/a} & 0.356 & 0.436 & 0.607 & 0.200 & 0.361 & 0.421 & \underline{\textbf{1.000}} & 0.481 \\
\midrule
\multicolumn{10}{l}{\textit{Lexical baseline}} \\ 
\midrule
\texttt{TF-IDF} & \texttt{n/a} & \textbf{0.190} & \textbf{0.205} & \textbf{0.518} & \textbf{0.091} & \textbf{0.401} & \textbf{0.474} & \underline{\textbf{1.000}} & \textbf{0.449} \\
\midrule
\bottomrule
\end{tabular}
}
\caption{\textbf{Primary-genre Success@10 on AURA-Bench (full results).}
Authorship representation is evaluated with Success@10 (S@10; higher is better). Values for \texttt{blog} and \texttt{social\_media} are macro-averaged over fine-grained subgenres.
Within each category block, best values are bolded (max for S@10). Across category blocks, best values are underlined.
\textbf{Size} denotes parameter count when publicly available; proprietary hosted models may not disclose sizes (marked \texttt{n/a}).}
\label{tab:genre-s10-full}
\end{table*}

\begin{table*}[h!]
\centering
\small
\setlength{\tabcolsep}{6pt}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccccccc}
\toprule
\textbf{Model} & \textbf{Size} & \textbf{\texttt{blog}} & \textbf{\texttt{ecommerce\_reviews}} & \textbf{\texttt{literature}} & \textbf{\texttt{media\_reviews}} & \textbf{\texttt{news}} & \textbf{\texttt{poetry}} & \textbf{\texttt{research\_paper}} & \textbf{\texttt{social\_media}} \\
\midrule
\multicolumn{10}{l}{\textit{LLMs (instruction-tuned)}} \\ 
\midrule
\texttt{llama3-8b-instruct} & 8B & 0.069 & \underline{\textbf{0.051}} & 0.046 & 0.091 & 0.087 & 0.024 & \underline{\textbf{0.000}} & \textbf{0.074} \\
\texttt{llama3.1-8b-instruct} & 8B & 0.072 & 0.053 & \underline{\textbf{0.045}} & 0.093 & 0.084 & \underline{\textbf{0.023}} & \underline{\textbf{0.000}} & 0.074 \\
\texttt{qwen2.5-3b-instruct} & 3B & 0.068 & 0.060 & 0.066 & \underline{\textbf{0.083}} & \underline{\textbf{0.063}} & 0.031 & 0.004 & 0.079 \\
\texttt{qwen2.5-7b-instruct} & 7B & \textbf{0.064} & 0.053 & 0.061 & 0.086 & 0.078 & 0.024 & 0.004 & 0.080 \\
\texttt{qwen3-4b-instruct} & 4B & 0.078 & 0.069 & 0.061 & 0.085 & 0.082 & \underline{\textbf{0.023}} & \underline{\textbf{0.000}} & 0.092 \\
\midrule
\multicolumn{10}{l}{\textit{LLMs (base)}} \\ 
\midrule
\texttt{llama3-8b} & 8B & \textbf{0.066} & \underline{\textbf{0.051}} & 0.050 & 0.090 & 0.085 & 0.024 & \textbf{0.004} & 0.074 \\
\texttt{llama3.1-8b} & 8B & 0.072 & \underline{\textbf{0.051}} & \textbf{0.050} & 0.095 & 0.085 & \underline{\textbf{0.023}} & \textbf{0.004} & \underline{\textbf{0.074}} \\
\texttt{qwen2.5-3b} & 3B & 0.071 & 0.066 & 0.068 & \textbf{0.088} & \textbf{0.067} & 0.029 & \textbf{0.004} & 0.081 \\
\texttt{qwen3-4b} & 4B & 0.070 & 0.055 & 0.060 & 0.092 & 0.082 & \underline{\textbf{0.023}} & \textbf{0.004} & 0.099 \\
\midrule
\multicolumn{10}{l}{\textit{Embedding models (instruction-tuned)}} \\ 
\midrule
\texttt{e5-mistral-7b-instruct} & 7B & 0.081 & 0.068 & \textbf{0.065} & 0.094 & 0.105 & 0.031 & \textbf{0.004} & \textbf{0.089} \\
\texttt{gte-qwen2-7b-instruct} & 7B & \underline{\textbf{0.060}} & \textbf{0.062} & 0.067 & \textbf{0.083} & \textbf{0.089} & \underline{\textbf{0.023}} & 0.008 & 0.090 \\
\midrule
\multicolumn{10}{l}{\textit{Embedding models}} \\ 
\midrule
\texttt{all-minilm-l12-v2} & \texttt{n/a} & 0.229 & 0.327 & 0.108 & 0.155 & 0.163 & 0.107 & \underline{\textbf{0.000}} & 0.150 \\
\texttt{all-minilm-l6-v2} & \texttt{n/a} & 0.236 & 0.333 & 0.083 & 0.148 & 0.152 & 0.136 & \underline{\textbf{0.000}} & 0.140 \\
\texttt{all-mpnet-base-v2} & \texttt{n/a} & 0.228 & 0.294 & 0.090 & 0.136 & 0.153 & 0.091 & \underline{\textbf{0.000}} & 0.143 \\
\texttt{all-roberta-large-v1} & \texttt{n/a} & 0.226 & 0.136 & 0.085 & 0.104 & 0.146 & 0.095 & \underline{\textbf{0.000}} & 0.099 \\
\texttt{allenai-specter} & \texttt{n/a} & 0.129 & 0.205 & 0.066 & 0.195 & 0.152 & 0.182 & \underline{\textbf{0.000}} & 0.183 \\
\texttt{bert-base-uncased} & \texttt{n/a} & 0.081 & 0.089 & 0.070 & 0.125 & 0.106 & 0.082 & \underline{\textbf{0.000}} & 0.134 \\
\texttt{bge-base-en-v1.5} & \texttt{n/a} & 0.365 & 0.385 & 0.096 & 0.165 & 0.195 & 0.118 & \underline{\textbf{0.000}} & 0.157 \\
\texttt{bge-base-zh-v1.5} & \texttt{n/a} & 0.149 & 0.154 & 0.073 & 0.247 & 0.139 & 0.091 & 0.004 & 0.145 \\
\texttt{bge-large-en-v1.5} & \texttt{n/a} & 0.315 & 0.359 & 0.095 & 0.144 & 0.178 & 0.091 & \underline{\textbf{0.000}} & 0.145 \\
\texttt{bge-large-zh-v1.5} & \texttt{n/a} & 0.171 & 0.245 & 0.078 & 0.300 & 0.132 & 0.091 & 0.004 & 0.167 \\
\texttt{bge-m3} & \texttt{n/a} & 0.297 & 0.282 & 0.118 & 0.345 & 0.296 & 0.114 & \underline{\textbf{0.000}} & 0.270 \\
\texttt{bge-small-en-v1.5} & \texttt{n/a} & 0.235 & 0.256 & 0.119 & 0.174 & 0.213 & 0.127 & \underline{\textbf{0.000}} & 0.166 \\
\texttt{distiluse-base-multilingual-cased-v2} & \texttt{n/a} & 0.232 & 0.138 & 0.158 & 0.326 & 0.216 & 0.101 & \underline{\textbf{0.000}} & 0.236 \\
\texttt{e5-base-v2} & \texttt{n/a} & 0.201 & 0.205 & 0.124 & 0.172 & 0.190 & 0.082 & \underline{\textbf{0.000}} & 0.162 \\
\texttt{e5-large-v2} & \texttt{n/a} & 0.248 & 0.154 & 0.141 & 0.207 & 0.201 & 0.103 & \underline{\textbf{0.000}} & 0.174 \\
\texttt{e5-small-v2} & \texttt{n/a} & 0.304 & 0.198 & 0.149 & 0.216 & 0.251 & 0.105 & 0.116 & 0.190 \\
\texttt{facebook-contriever} & \texttt{n/a} & 0.308 & 0.097 & 0.073 & 0.126 & 0.118 & 0.068 & 0.200 & 0.138 \\
\texttt{facebook-contriever-msmarco} & \texttt{n/a} & 0.254 & 0.154 & 0.107 & 0.129 & 0.206 & 0.091 & 0.004 & 0.167 \\
\texttt{gte-base} & \texttt{n/a} & 0.267 & 0.359 & 0.085 & 0.144 & 0.172 & 0.136 & \underline{\textbf{0.000}} & 0.136 \\
\texttt{gte-large} & \texttt{n/a} & 0.208 & 0.366 & 0.079 & 0.126 & 0.136 & 0.114 & \underline{\textbf{0.000}} & 0.119 \\
\texttt{gte-large-en-v1.5} & \texttt{n/a} & 0.150 & 0.410 & 0.057 & 0.138 & 0.107 & 0.108 & \underline{\textbf{0.000}} & 0.100 \\
\texttt{jina-embeddings-v2-base-en} & \texttt{n/a} & 0.352 & 0.333 & 0.253 & 0.224 & 0.299 & 0.068 & 0.300 & 0.259 \\
\texttt{jina-embeddings-v2-small-en} & \texttt{n/a} & 0.105 & 0.092 & 0.109 & 0.117 & 0.100 & 0.105 & 0.016 & 0.140 \\
\texttt{msmarco-distilbert-base-v4} & \texttt{n/a} & 0.353 & 0.205 & 0.141 & 0.138 & 0.261 & 0.136 & \underline{\textbf{0.000}} & 0.197 \\
\texttt{multilingual-e5-base} & \texttt{n/a} & 0.155 & 0.128 & 0.107 & 0.103 & 0.139 & 0.075 & \underline{\textbf{0.000}} & 0.131 \\
\texttt{multilingual-e5-large} & \texttt{n/a} & 0.142 & 0.103 & 0.090 & \textbf{0.087} & 0.129 & 0.068 & \underline{\textbf{0.000}} & 0.125 \\
\texttt{mxbai-embed-large-v1} & \texttt{n/a} & 0.322 & 0.355 & 0.090 & 0.138 & 0.188 & 0.092 & \underline{\textbf{0.000}} & 0.146 \\
\texttt{nomic-embed-text-v1} & \texttt{n/a} & 0.172 & 0.256 & 0.070 & 0.190 & 0.155 & 0.103 & \underline{\textbf{0.000}} & 0.150 \\
\texttt{nomic-embed-text-v1.5} & \texttt{n/a} & 0.188 & 0.205 & 0.065 & 0.157 & 0.129 & 0.102 & \underline{\textbf{0.000}} & 0.141 \\
\texttt{paraphrase-mpnet-base-v2} & \texttt{n/a} & 0.164 & 0.103 & 0.079 & 0.138 & 0.157 & 0.091 & \underline{\textbf{0.000}} & 0.152 \\
\texttt{paraphrase-multilingual-mpnet-base-v2} & \texttt{n/a} & 0.297 & 0.230 & 0.181 & 0.207 & 0.310 & 0.117 & 0.012 & 0.247 \\
\texttt{qwen3-embedding-0.6b} & 0.6B & 0.090 & 0.096 & 0.059 & 0.119 & \textbf{0.094} & 0.041 & \underline{\textbf{0.000}} & 0.093 \\
\texttt{qwen3-embedding-4b} & 4B & 0.092 & 0.077 & \textbf{0.051} & 0.155 & 0.101 & 0.044 & \underline{\textbf{0.000}} & 0.091 \\
\texttt{qwen3-embedding-8b} & 8B & 0.116 & 0.077 & 0.062 & 0.103 & 0.118 & 0.039 & \underline{\textbf{0.000}} & \textbf{0.088} \\
\texttt{sfr-embedding-mistral} & 7B & \textbf{0.067} & \textbf{0.066} & 0.064 & 0.094 & 0.105 & \textbf{0.028} & 0.004 & 0.089 \\
\texttt{snowflake-arctic-embed-l-v2} & \texttt{n/a} & 0.246 & 0.179 & 0.097 & 0.224 & 0.221 & 0.042 & \underline{\textbf{0.000}} & 0.175 \\
\midrule
\multicolumn{10}{l}{\textit{Lexical baseline}} \\ 
\midrule
\texttt{TF-IDF} & \texttt{n/a} & \textbf{0.106} & \textbf{0.179} & \textbf{0.107} & \textbf{0.689} & \textbf{0.153} & \textbf{0.095} & \underline{\textbf{0.000}} & \textbf{0.251} \\
\midrule
\bottomrule
\end{tabular}
}
\caption{\textbf{Primary-genre EER@10 on AURA-Bench (full results).}
Authorship verification is evaluated with equal error rate (EER@10; lower is better). Values for \texttt{blog} and \texttt{social\_media} are macro-averaged over fine-grained subgenres.
Within each category block, best values are bolded (min for EER@10). Across category blocks, best values are underlined.
\textbf{Size} denotes parameter count when publicly available; proprietary hosted models may not disclose sizes (marked \texttt{n/a}).}
\label{tab:genre-eer-full}
\end{table*}

\begin{table*}[h!]
\centering
\small
\setlength{\tabcolsep}{6pt}
\resizebox{0.85\textwidth}{!}{%
\begin{tabular}{lcccccc}
\toprule
\textbf{Model} & \textbf{Size} & \textbf{\texttt{short}} & \textbf{\texttt{medium}} & \textbf{\texttt{long}} & \textbf{\texttt{extra\_long}} \\
\midrule
\multicolumn{6}{l}{\textit{LLMs (instruction-tuned)}} \\ 
\midrule
\texttt{llama3-8b-instruct} & 8B & 0.458 & \textbf{0.605} & \textbf{0.742} & \underline{\textbf{1.000}} \\
\texttt{llama3.1-8b-instruct} & 8B & \textbf{0.486} & 0.602 & 0.729 & \underline{\textbf{1.000}} \\
\texttt{qwen2.5-3b-instruct} & 3B & 0.375 & 0.553 & 0.706 & 0.250 \\
\texttt{qwen2.5-7b-instruct} & 7B & 0.361 & 0.580 & 0.693 & 0.750 \\
\texttt{qwen3-4b-instruct} & 4B & 0.389 & 0.553 & 0.688 & 0.250 \\
\midrule
\multicolumn{6}{l}{\textit{LLMs (base)}} \\ 
\midrule
\texttt{llama3-8b} & 8B & 0.486 & \underline{\textbf{0.621}} & 0.742 & \underline{\textbf{1.000}} \\
\texttt{llama3.1-8b} & 8B & \underline{\textbf{0.500}} & 0.613 & \underline{\textbf{0.747}} & \underline{\textbf{1.000}} \\
\texttt{qwen2.5-3b} & 3B & 0.403 & 0.551 & 0.698 & 0.250 \\
\texttt{qwen3-4b} & 4B & 0.389 & 0.555 & 0.708 & 0.500 \\
\midrule
\multicolumn{6}{l}{\textit{Embedding models (instruction-tuned)}} \\ 
\midrule
\texttt{e5-mistral-7b-instruct} & 7B & \textbf{0.417} & 0.551 & 0.711 & \textbf{0.750} \\
\texttt{gte-qwen2-7b-instruct} & 7B & 0.375 & \textbf{0.576} & \textbf{0.716} & 0.500 \\
\midrule
\multicolumn{6}{l}{\textit{Embedding models}} \\ 
\midrule
\texttt{all-minilm-l12-v2} & \texttt{n/a} & 0.403 & 0.379 & 0.479 & 0.250 \\
\texttt{all-minilm-l6-v2} & \texttt{n/a} & 0.361 & 0.371 & 0.487 & 0.500 \\
\texttt{all-mpnet-base-v2} & \texttt{n/a} & 0.431 & 0.412 & 0.464 & \textbf{0.750} \\
\texttt{all-roberta-large-v1} & \texttt{n/a} & 0.403 & 0.471 & 0.536 & 0.500 \\
\texttt{allenai-specter} & \texttt{n/a} & 0.264 & 0.324 & 0.497 & 0.250 \\
\texttt{bert-base-uncased} & \texttt{n/a} & 0.361 & 0.434 & 0.602 & 0.500 \\
\texttt{bge-base-en-v1.5} & \texttt{n/a} & 0.375 & 0.371 & 0.549 & 0.250 \\
\texttt{bge-base-zh-v1.5} & \texttt{n/a} & 0.403 & 0.414 & 0.568 & 0.250 \\
\texttt{bge-large-en-v1.5} & \texttt{n/a} & 0.292 & 0.387 & 0.536 & 0.500 \\
\texttt{bge-large-zh-v1.5} & \texttt{n/a} & 0.417 & 0.404 & 0.513 & 0.500 \\
\texttt{bge-m3} & \texttt{n/a} & 0.222 & 0.318 & 0.523 & 0.250 \\
\texttt{bge-small-en-v1.5} & \texttt{n/a} & 0.347 & 0.383 & 0.474 & 0.500 \\
\texttt{distiluse-base-multilingual-cased-v2} & \texttt{n/a} & 0.278 & 0.363 & 0.495 & 0.500 \\
\texttt{e5-base-v2} & \texttt{n/a} & 0.389 & 0.467 & 0.578 & \textbf{0.750} \\
\texttt{e5-large-v2} & \texttt{n/a} & 0.375 & 0.445 & 0.591 & 0.250 \\
\texttt{e5-small-v2} & \texttt{n/a} & 0.306 & 0.434 & 0.536 & 0.500 \\
\texttt{facebook-contriever} & \texttt{n/a} & 0.361 & 0.514 & 0.630 & \textbf{0.750} \\
\texttt{facebook-contriever-msmarco} & \texttt{n/a} & 0.319 & 0.428 & 0.523 & 0.250 \\
\texttt{gte-base} & \texttt{n/a} & 0.333 & 0.383 & 0.487 & 0.250 \\
\texttt{gte-large} & \texttt{n/a} & 0.347 & 0.412 & 0.547 & 0.500 \\
\texttt{gte-large-en-v1.5} & \texttt{n/a} & 0.347 & 0.434 & 0.594 & 0.500 \\
\texttt{jina-embeddings-v2-base-en} & \texttt{n/a} & 0.208 & 0.164 & 0.224 & 0.250 \\
\texttt{jina-embeddings-v2-small-en} & \texttt{n/a} & 0.375 & 0.379 & 0.430 & 0.500 \\
\texttt{msmarco-distilbert-base-v4} & \texttt{n/a} & 0.403 & 0.350 & 0.479 & 0.250 \\
\texttt{multilingual-e5-base} & \texttt{n/a} & 0.431 & 0.543 & 0.638 & 0.250 \\
\texttt{multilingual-e5-large} & \texttt{n/a} & 0.417 & \textbf{0.547} & 0.661 & 0.500 \\
\texttt{mxbai-embed-large-v1} & \texttt{n/a} & 0.292 & 0.377 & 0.518 & 0.500 \\
\texttt{nomic-embed-text-v1} & \texttt{n/a} & 0.375 & 0.420 & 0.576 & 0.500 \\
\texttt{nomic-embed-text-v1.5} & \texttt{n/a} & 0.333 & 0.418 & 0.542 & 0.500 \\
\texttt{paraphrase-mpnet-base-v2} & \texttt{n/a} & \textbf{0.458} & 0.412 & 0.487 & 0.500 \\
\texttt{paraphrase-multilingual-mpnet-base-v2} & \texttt{n/a} & 0.222 & 0.309 & 0.424 & 0.250 \\
\texttt{qwen3-embedding-0.6b} & 0.6B & 0.278 & 0.512 & 0.628 & 0.250 \\
\texttt{qwen3-embedding-4b} & 4B & 0.361 & 0.510 & 0.669 & 0.500 \\
\texttt{qwen3-embedding-8b} & 8B & 0.347 & 0.541 & \textbf{0.698} & 0.500 \\
\texttt{sfr-embedding-mistral} & 7B & 0.431 & 0.535 & 0.695 & \textbf{0.750} \\
\texttt{snowflake-arctic-embed-l-v2} & \texttt{n/a} & 0.306 & 0.375 & 0.555 & 0.500 \\
\midrule
\multicolumn{6}{l}{\textit{Lexical baseline}} \\ 
\midrule
\texttt{TF-IDF} & \texttt{n/a} & \textbf{0.208} & \textbf{0.334} & \textbf{0.549} & \textbf{0.250} \\
\midrule
\bottomrule
\end{tabular}
}
\caption{\textbf{Length-bucket Success@10 on AURA-Bench (full results).}
Authorship representation is evaluated with Success@10 (S@10; higher is better).
Within each category block, best values are bolded (max for S@10). Across category blocks, best values are underlined.
\textbf{Size} denotes parameter count when publicly available; proprietary hosted models may not disclose sizes (marked \texttt{n/a}).}
\label{tab:length-s10-full}
\end{table*}

\begin{table*}[h!]
\centering
\small
\setlength{\tabcolsep}{6pt}
\resizebox{0.85\textwidth}{!}{%
\begin{tabular}{lcccccc}
\toprule
\textbf{Model} & \textbf{Size} & \textbf{\texttt{short}} & \textbf{\texttt{medium}} & \textbf{\texttt{long}} & \textbf{\texttt{extra\_long}} \\
\midrule
\multicolumn{6}{l}{\textit{LLMs (instruction-tuned)}} \\ 
\midrule
\texttt{llama3-8b-instruct} & 8B & 0.107 & 0.087 & \underline{\textbf{0.060}} & \underline{\textbf{0.005}} \\
\texttt{llama3.1-8b-instruct} & 8B & \underline{\textbf{0.103}} & 0.085 & 0.063 & \underline{\textbf{0.005}} \\
\texttt{qwen2.5-3b-instruct} & 3B & 0.108 & \underline{\textbf{0.075}} & 0.066 & 0.030 \\
\texttt{qwen2.5-7b-instruct} & 7B & 0.104 & 0.078 & 0.071 & 0.025 \\
\texttt{qwen3-4b-instruct} & 4B & 0.120 & 0.089 & 0.080 & 0.050 \\
\midrule
\multicolumn{6}{l}{\textit{LLMs (base)}} \\ 
\midrule
\texttt{llama3-8b} & 8B & \textbf{0.106} & 0.089 & 0.067 & \underline{\textbf{0.005}} \\
\texttt{llama3.1-8b} & 8B & 0.110 & 0.089 & \textbf{0.066} & \underline{\textbf{0.005}} \\
\texttt{qwen2.5-3b} & 3B & 0.107 & \textbf{0.078} & 0.069 & 0.030 \\
\texttt{qwen3-4b} & 4B & 0.111 & 0.085 & 0.082 & 0.045 \\
\midrule
\multicolumn{6}{l}{\textit{Embedding models (instruction-tuned)}} \\ 
\midrule
\texttt{e5-mistral-7b-instruct} & 7B & 0.121 & 0.102 & \textbf{0.083} & 0.040 \\
\texttt{gte-qwen2-7b-instruct} & 7B & \textbf{0.106} & \textbf{0.087} & 0.084 & \textbf{0.020} \\
\midrule
\multicolumn{6}{l}{\textit{Embedding models}} \\ 
\midrule
\texttt{all-minilm-l12-v2} & \texttt{n/a} & 0.240 & 0.186 & 0.115 & 0.285 \\
\texttt{all-minilm-l6-v2} & \texttt{n/a} & 0.267 & 0.176 & 0.105 & 0.250 \\
\texttt{all-mpnet-base-v2} & \texttt{n/a} & 0.240 & 0.166 & 0.108 & 0.250 \\
\texttt{all-roberta-large-v1} & \texttt{n/a} & 0.227 & 0.133 & 0.101 & 0.250 \\
\texttt{allenai-specter} & \texttt{n/a} & 0.272 & 0.167 & 0.125 & \textbf{0.040} \\
\texttt{bert-base-uncased} & \texttt{n/a} & 0.160 & 0.104 & 0.090 & 0.060 \\
\texttt{bge-base-en-v1.5} & \texttt{n/a} & 0.333 & 0.205 & 0.116 & 0.350 \\
\texttt{bge-base-zh-v1.5} & \texttt{n/a} & 0.212 & 0.150 & 0.104 & \textbf{0.040} \\
\texttt{bge-large-en-v1.5} & \texttt{n/a} & 0.333 & 0.185 & 0.105 & 0.315 \\
\texttt{bge-large-zh-v1.5} & \texttt{n/a} & 0.199 & 0.160 & 0.100 & 0.070 \\
\texttt{bge-m3} & \texttt{n/a} & 0.360 & 0.281 & 0.214 & 0.250 \\
\texttt{bge-small-en-v1.5} & \texttt{n/a} & 0.320 & 0.218 & 0.152 & 0.250 \\
\texttt{distiluse-base-multilingual-cased-v2} & \texttt{n/a} & 0.230 & 0.233 & 0.169 & 0.225 \\
\texttt{e5-base-v2} & \texttt{n/a} & 0.267 & 0.191 & 0.130 & 0.250 \\
\texttt{e5-large-v2} & \texttt{n/a} & 0.261 & 0.195 & 0.147 & 0.205 \\
\texttt{e5-small-v2} & \texttt{n/a} & 0.293 & 0.242 & 0.162 & 0.250 \\
\texttt{facebook-contriever} & \texttt{n/a} & 0.278 & 0.136 & 0.101 & 0.250 \\
\texttt{facebook-contriever-msmarco} & \texttt{n/a} & 0.281 & 0.193 & 0.125 & 0.465 \\
\texttt{gte-base} & \texttt{n/a} & 0.293 & 0.175 & 0.106 & 0.265 \\
\texttt{gte-large} & \texttt{n/a} & 0.280 & 0.152 & 0.092 & 0.290 \\
\texttt{gte-large-en-v1.5} & \texttt{n/a} & 0.213 & 0.133 & 0.088 & 0.250 \\
\texttt{jina-embeddings-v2-base-en} & \texttt{n/a} & 0.360 & 0.312 & 0.273 & 0.250 \\
\texttt{jina-embeddings-v2-small-en} & \texttt{n/a} & 0.151 & 0.108 & 0.108 & 0.045 \\
\texttt{msmarco-distilbert-base-v4} & \texttt{n/a} & 0.293 & 0.259 & 0.184 & 0.355 \\
\texttt{multilingual-e5-base} & \texttt{n/a} & 0.173 & 0.166 & 0.131 & 0.250 \\
\texttt{multilingual-e5-large} & \texttt{n/a} & 0.177 & 0.153 & 0.130 & 0.135 \\
\texttt{mxbai-embed-large-v1} & \texttt{n/a} & 0.333 & 0.197 & 0.107 & 0.285 \\
\texttt{nomic-embed-text-v1} & \texttt{n/a} & 0.278 & 0.176 & 0.099 & 0.250 \\
\texttt{nomic-embed-text-v1.5} & \texttt{n/a} & 0.267 & 0.154 & 0.092 & 0.250 \\
\texttt{paraphrase-mpnet-base-v2} & \texttt{n/a} & 0.280 & 0.165 & 0.112 & 0.320 \\
\texttt{paraphrase-multilingual-mpnet-base-v2} & \texttt{n/a} & 0.307 & 0.277 & 0.227 & 0.500 \\
\texttt{qwen3-embedding-0.6b} & 0.6B & 0.173 & 0.104 & 0.099 & 0.135 \\
\texttt{qwen3-embedding-4b} & 4B & 0.161 & 0.108 & 0.096 & 0.055 \\
\texttt{qwen3-embedding-8b} & 8B & 0.176 & 0.118 & 0.120 & 0.100 \\
\texttt{sfr-embedding-mistral} & 7B & \textbf{0.120} & \textbf{0.101} & \textbf{0.083} & 0.050 \\
\texttt{snowflake-arctic-embed-l-v2} & \texttt{n/a} & 0.265 & 0.212 & 0.167 & 0.250 \\
\midrule
\multicolumn{6}{l}{\textit{Lexical baseline}} \\ 
\midrule
\texttt{TF-IDF} & \texttt{n/a} & \textbf{0.453} & \textbf{0.188} & \textbf{0.135} & \textbf{0.050} \\
\midrule
\bottomrule
\end{tabular}
}
\caption{\textbf{Length-bucket EER@10 on AURA-Bench (full results).}
Authorship verification is evaluated with equal error rate (EER@10; lower is better).
Within each category block, best values are bolded (min for EER@10). Across category blocks, best values are underlined.
\textbf{Size} denotes parameter count when publicly available; proprietary hosted models may not disclose sizes (marked \texttt{n/a}).}
\label{tab:length-eer-full}
\end{table*}
